{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Python_Exercise_3.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"TuuLgt5mW6rS","colab_type":"text"},"source":["#Python Exercise 3\n","#Samuel Statton"]},{"cell_type":"code","metadata":{"id":"_X34fpNUHq5s","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":122},"outputId":"32adf399-064f-4c05-9f02-6084d049b4d1","executionInfo":{"status":"ok","timestamp":1574812320216,"user_tz":420,"elapsed":20069,"user":{"displayName":"Samuel Statton","photoUrl":"","userId":"01725452895570387843"}}},"source":["from google.colab import drive \n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MvKX4Sj8IW_k","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LassoLarsCV\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N98AIlBxIfR6","colab_type":"code","colab":{}},"source":["alldata = pd.read_csv('drive/My Drive/Colab Notebooks/finalmaster-ratios.csv')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pzKwBmBSIq_x","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":270},"outputId":"8b0edf32-62ee-473d-938a-20432ac73b99","executionInfo":{"status":"ok","timestamp":1574812436754,"user_tz":420,"elapsed":556,"user":{"displayName":"Samuel Statton","photoUrl":"","userId":"01725452895570387843"}}},"source":["alldata.head()"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th># Purchases</th>\n","      <th>B01001001</th>\n","      <th>B01001002</th>\n","      <th>B01001003</th>\n","      <th>B01001004</th>\n","      <th>B01001005</th>\n","      <th>B01001006</th>\n","      <th>B01001007</th>\n","      <th>B01001008</th>\n","      <th>B01001009</th>\n","      <th>B01001010</th>\n","      <th>B01001011</th>\n","      <th>B01001012</th>\n","      <th>B01001013</th>\n","      <th>B01001014</th>\n","      <th>B01001015</th>\n","      <th>B01001016</th>\n","      <th>B01001017</th>\n","      <th>B01001018</th>\n","      <th>B01001019</th>\n","      <th>B01001020</th>\n","      <th>B01001021</th>\n","      <th>B01001022</th>\n","      <th>B01001023</th>\n","      <th>B01001024</th>\n","      <th>B01001025</th>\n","      <th>B01001026</th>\n","      <th>B01001027</th>\n","      <th>B01001028</th>\n","      <th>B01001029</th>\n","      <th>B01001030</th>\n","      <th>B01001031</th>\n","      <th>B01001032</th>\n","      <th>B01001033</th>\n","      <th>B01001034</th>\n","      <th>B01001035</th>\n","      <th>B01001036</th>\n","      <th>B01001037</th>\n","      <th>B01001038</th>\n","      <th>B01001039</th>\n","      <th>...</th>\n","      <th>B15002013</th>\n","      <th>B15002014</th>\n","      <th>B15002015</th>\n","      <th>B15002016</th>\n","      <th>B15002017</th>\n","      <th>B15002018</th>\n","      <th>B15002019</th>\n","      <th>B15002020</th>\n","      <th>B15002021</th>\n","      <th>B15002022</th>\n","      <th>B15002023</th>\n","      <th>B15002024</th>\n","      <th>B15002025</th>\n","      <th>B15002026</th>\n","      <th>B15002027</th>\n","      <th>B15002028</th>\n","      <th>B15002029</th>\n","      <th>B15002030</th>\n","      <th>B15002031</th>\n","      <th>B15002032</th>\n","      <th>B15002033</th>\n","      <th>B15002034</th>\n","      <th>B15002035</th>\n","      <th>B19001001</th>\n","      <th>B19001002</th>\n","      <th>B19001003</th>\n","      <th>B19001004</th>\n","      <th>B19001005</th>\n","      <th>B19001006</th>\n","      <th>B19001007</th>\n","      <th>B19001008</th>\n","      <th>B19001009</th>\n","      <th>B19001010</th>\n","      <th>B19001011</th>\n","      <th>B19001012</th>\n","      <th>B19001013</th>\n","      <th>B19001014</th>\n","      <th>B19001015</th>\n","      <th>B19001016</th>\n","      <th>B19001017</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>22</td>\n","      <td>206252</td>\n","      <td>469.226965</td>\n","      <td>31.432422</td>\n","      <td>35.219052</td>\n","      <td>33.628765</td>\n","      <td>20.121017</td>\n","      <td>12.610787</td>\n","      <td>6.734480</td>\n","      <td>6.225394</td>\n","      <td>19.432539</td>\n","      <td>28.101546</td>\n","      <td>28.421543</td>\n","      <td>26.390047</td>\n","      <td>31.989993</td>\n","      <td>31.359696</td>\n","      <td>32.116052</td>\n","      <td>32.213021</td>\n","      <td>12.184124</td>\n","      <td>18.361034</td>\n","      <td>9.454454</td>\n","      <td>15.175610</td>\n","      <td>16.281054</td>\n","      <td>11.025348</td>\n","      <td>6.230243</td>\n","      <td>4.518744</td>\n","      <td>530.773035</td>\n","      <td>31.999690</td>\n","      <td>34.322091</td>\n","      <td>32.649380</td>\n","      <td>20.101623</td>\n","      <td>12.513818</td>\n","      <td>8.072649</td>\n","      <td>6.021760</td>\n","      <td>22.923414</td>\n","      <td>31.335454</td>\n","      <td>31.558482</td>\n","      <td>31.063941</td>\n","      <td>36.082074</td>\n","      <td>34.845723</td>\n","      <td>...</td>\n","      <td>64.610300</td>\n","      <td>31.449746</td>\n","      <td>58.735313</td>\n","      <td>20.071053</td>\n","      <td>6.726751</td>\n","      <td>5.882267</td>\n","      <td>543.803963</td>\n","      <td>6.974272</td>\n","      <td>2.504332</td>\n","      <td>5.904107</td>\n","      <td>11.917415</td>\n","      <td>10.767170</td>\n","      <td>18.141844</td>\n","      <td>19.779852</td>\n","      <td>10.956451</td>\n","      <td>181.418442</td>\n","      <td>26.717724</td>\n","      <td>85.271036</td>\n","      <td>54.243532</td>\n","      <td>72.647457</td>\n","      <td>30.816383</td>\n","      <td>2.831933</td>\n","      <td>2.912014</td>\n","      <td>1000</td>\n","      <td>105.667996</td>\n","      <td>82.298375</td>\n","      <td>68.141163</td>\n","      <td>67.336195</td>\n","      <td>63.566902</td>\n","      <td>59.439845</td>\n","      <td>49.409690</td>\n","      <td>53.306757</td>\n","      <td>42.318307</td>\n","      <td>83.167229</td>\n","      <td>89.249208</td>\n","      <td>102.141470</td>\n","      <td>52.872330</td>\n","      <td>36.440765</td>\n","      <td>23.446284</td>\n","      <td>21.197485</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>7</td>\n","      <td>61399</td>\n","      <td>486.538869</td>\n","      <td>22.899396</td>\n","      <td>21.531295</td>\n","      <td>27.036271</td>\n","      <td>16.808091</td>\n","      <td>28.355511</td>\n","      <td>18.192479</td>\n","      <td>13.534422</td>\n","      <td>21.466148</td>\n","      <td>24.886399</td>\n","      <td>23.534585</td>\n","      <td>21.319565</td>\n","      <td>27.101419</td>\n","      <td>30.961416</td>\n","      <td>37.117868</td>\n","      <td>36.466392</td>\n","      <td>12.557208</td>\n","      <td>20.554081</td>\n","      <td>12.182609</td>\n","      <td>15.651721</td>\n","      <td>20.668089</td>\n","      <td>15.961172</td>\n","      <td>10.423623</td>\n","      <td>7.329110</td>\n","      <td>513.461131</td>\n","      <td>18.974250</td>\n","      <td>23.404290</td>\n","      <td>23.892897</td>\n","      <td>17.036108</td>\n","      <td>35.310021</td>\n","      <td>18.534504</td>\n","      <td>17.101256</td>\n","      <td>22.785387</td>\n","      <td>22.150198</td>\n","      <td>22.622518</td>\n","      <td>21.303279</td>\n","      <td>26.971123</td>\n","      <td>32.329517</td>\n","      <td>...</td>\n","      <td>56.929829</td>\n","      <td>46.381727</td>\n","      <td>65.707446</td>\n","      <td>35.509451</td>\n","      <td>16.782205</td>\n","      <td>9.201536</td>\n","      <td>515.086529</td>\n","      <td>3.017306</td>\n","      <td>1.047329</td>\n","      <td>1.371503</td>\n","      <td>6.358785</td>\n","      <td>4.937410</td>\n","      <td>8.303825</td>\n","      <td>9.700264</td>\n","      <td>7.555733</td>\n","      <td>174.155902</td>\n","      <td>25.834123</td>\n","      <td>60.146626</td>\n","      <td>62.440776</td>\n","      <td>76.604658</td>\n","      <td>55.383771</td>\n","      <td>8.977108</td>\n","      <td>9.251409</td>\n","      <td>1000</td>\n","      <td>71.289558</td>\n","      <td>59.062447</td>\n","      <td>54.704688</td>\n","      <td>60.966323</td>\n","      <td>53.012354</td>\n","      <td>60.881706</td>\n","      <td>59.231680</td>\n","      <td>50.093078</td>\n","      <td>40.700626</td>\n","      <td>92.612963</td>\n","      <td>117.363344</td>\n","      <td>113.344051</td>\n","      <td>75.774243</td>\n","      <td>33.000508</td>\n","      <td>33.169741</td>\n","      <td>24.792689</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>73170</td>\n","      <td>489.859232</td>\n","      <td>28.905289</td>\n","      <td>36.271696</td>\n","      <td>28.235616</td>\n","      <td>21.566216</td>\n","      <td>12.218122</td>\n","      <td>7.243406</td>\n","      <td>7.380074</td>\n","      <td>16.933169</td>\n","      <td>24.914582</td>\n","      <td>26.896269</td>\n","      <td>31.802651</td>\n","      <td>30.531639</td>\n","      <td>36.258029</td>\n","      <td>35.998360</td>\n","      <td>33.429001</td>\n","      <td>13.625803</td>\n","      <td>19.406861</td>\n","      <td>12.245456</td>\n","      <td>14.664480</td>\n","      <td>21.169878</td>\n","      <td>15.293153</td>\n","      <td>8.610086</td>\n","      <td>6.259396</td>\n","      <td>510.140768</td>\n","      <td>26.171928</td>\n","      <td>30.681973</td>\n","      <td>31.925653</td>\n","      <td>19.789531</td>\n","      <td>10.072434</td>\n","      <td>5.056717</td>\n","      <td>6.218396</td>\n","      <td>15.757824</td>\n","      <td>24.449911</td>\n","      <td>26.595599</td>\n","      <td>27.210605</td>\n","      <td>37.556376</td>\n","      <td>37.050704</td>\n","      <td>...</td>\n","      <td>54.602613</td>\n","      <td>40.613027</td>\n","      <td>43.363788</td>\n","      <td>12.280185</td>\n","      <td>5.796247</td>\n","      <td>3.438452</td>\n","      <td>523.980745</td>\n","      <td>5.422930</td>\n","      <td>4.224384</td>\n","      <td>11.828274</td>\n","      <td>18.331860</td>\n","      <td>15.089891</td>\n","      <td>21.731015</td>\n","      <td>18.685529</td>\n","      <td>7.014441</td>\n","      <td>155.241183</td>\n","      <td>45.466156</td>\n","      <td>71.185775</td>\n","      <td>65.802142</td>\n","      <td>56.272718</td>\n","      <td>24.580018</td>\n","      <td>1.689753</td>\n","      <td>1.414677</td>\n","      <td>1000</td>\n","      <td>102.538696</td>\n","      <td>82.960331</td>\n","      <td>74.828305</td>\n","      <td>79.133495</td>\n","      <td>66.081252</td>\n","      <td>78.245122</td>\n","      <td>63.996993</td>\n","      <td>47.322923</td>\n","      <td>42.505211</td>\n","      <td>70.420610</td>\n","      <td>90.033143</td>\n","      <td>98.677692</td>\n","      <td>54.703249</td>\n","      <td>20.125056</td>\n","      <td>11.890525</td>\n","      <td>16.537397</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>94</td>\n","      <td>251724</td>\n","      <td>505.585483</td>\n","      <td>32.054949</td>\n","      <td>31.757004</td>\n","      <td>28.102207</td>\n","      <td>18.651380</td>\n","      <td>12.080692</td>\n","      <td>7.035483</td>\n","      <td>7.686991</td>\n","      <td>25.790151</td>\n","      <td>42.129475</td>\n","      <td>35.824951</td>\n","      <td>32.058922</td>\n","      <td>27.677138</td>\n","      <td>33.842621</td>\n","      <td>38.176733</td>\n","      <td>32.722347</td>\n","      <td>12.493842</td>\n","      <td>16.394940</td>\n","      <td>11.504664</td>\n","      <td>15.914255</td>\n","      <td>16.394940</td>\n","      <td>13.196994</td>\n","      <td>8.648361</td>\n","      <td>5.446441</td>\n","      <td>494.414517</td>\n","      <td>33.123580</td>\n","      <td>28.082344</td>\n","      <td>30.171934</td>\n","      <td>16.863708</td>\n","      <td>9.280005</td>\n","      <td>5.390825</td>\n","      <td>5.609318</td>\n","      <td>19.453846</td>\n","      <td>35.614403</td>\n","      <td>32.082757</td>\n","      <td>28.809331</td>\n","      <td>27.911522</td>\n","      <td>32.690566</td>\n","      <td>...</td>\n","      <td>88.227492</td>\n","      <td>44.076261</td>\n","      <td>87.939148</td>\n","      <td>44.404973</td>\n","      <td>9.671057</td>\n","      <td>7.283569</td>\n","      <td>502.912274</td>\n","      <td>4.509700</td>\n","      <td>0.980370</td>\n","      <td>3.552398</td>\n","      <td>5.986021</td>\n","      <td>7.398907</td>\n","      <td>9.740260</td>\n","      <td>10.605292</td>\n","      <td>7.485410</td>\n","      <td>141.242417</td>\n","      <td>43.078591</td>\n","      <td>84.479020</td>\n","      <td>52.069156</td>\n","      <td>89.836451</td>\n","      <td>33.932320</td>\n","      <td>4.129086</td>\n","      <td>3.886877</td>\n","      <td>1000</td>\n","      <td>61.632139</td>\n","      <td>46.526521</td>\n","      <td>48.437595</td>\n","      <td>54.221644</td>\n","      <td>51.680322</td>\n","      <td>60.066684</td>\n","      <td>54.790900</td>\n","      <td>48.681562</td>\n","      <td>43.873381</td>\n","      <td>84.717507</td>\n","      <td>112.204444</td>\n","      <td>127.137252</td>\n","      <td>83.019904</td>\n","      <td>43.731067</td>\n","      <td>38.851729</td>\n","      <td>40.427349</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>37382</td>\n","      <td>495.586111</td>\n","      <td>25.413301</td>\n","      <td>29.318924</td>\n","      <td>26.162324</td>\n","      <td>19.260607</td>\n","      <td>12.893906</td>\n","      <td>6.580707</td>\n","      <td>7.062222</td>\n","      <td>17.334546</td>\n","      <td>32.930287</td>\n","      <td>28.302392</td>\n","      <td>28.569900</td>\n","      <td>26.804344</td>\n","      <td>30.549462</td>\n","      <td>36.595153</td>\n","      <td>42.373335</td>\n","      <td>16.398267</td>\n","      <td>22.871970</td>\n","      <td>17.174041</td>\n","      <td>15.221229</td>\n","      <td>23.433738</td>\n","      <td>14.391953</td>\n","      <td>7.383233</td>\n","      <td>8.560270</td>\n","      <td>504.413889</td>\n","      <td>26.563587</td>\n","      <td>30.255203</td>\n","      <td>24.798031</td>\n","      <td>16.237761</td>\n","      <td>11.101600</td>\n","      <td>4.788401</td>\n","      <td>5.189663</td>\n","      <td>17.842812</td>\n","      <td>30.014445</td>\n","      <td>27.767375</td>\n","      <td>30.763469</td>\n","      <td>25.199294</td>\n","      <td>29.613183</td>\n","      <td>...</td>\n","      <td>102.957039</td>\n","      <td>36.711921</td>\n","      <td>70.039055</td>\n","      <td>33.587502</td>\n","      <td>5.021387</td>\n","      <td>5.244560</td>\n","      <td>511.177236</td>\n","      <td>2.045750</td>\n","      <td>3.236005</td>\n","      <td>1.525014</td>\n","      <td>7.476288</td>\n","      <td>4.314674</td>\n","      <td>8.554956</td>\n","      <td>13.204389</td>\n","      <td>7.773852</td>\n","      <td>130.890831</td>\n","      <td>53.784638</td>\n","      <td>99.311884</td>\n","      <td>57.095034</td>\n","      <td>76.027525</td>\n","      <td>38.422912</td>\n","      <td>4.351869</td>\n","      <td>3.161614</td>\n","      <td>1000</td>\n","      <td>51.125525</td>\n","      <td>58.438255</td>\n","      <td>68.930434</td>\n","      <td>74.717029</td>\n","      <td>63.970495</td>\n","      <td>59.710034</td>\n","      <td>58.883378</td>\n","      <td>51.761414</td>\n","      <td>47.310187</td>\n","      <td>81.902582</td>\n","      <td>93.793717</td>\n","      <td>130.103014</td>\n","      <td>71.982704</td>\n","      <td>36.118530</td>\n","      <td>31.603714</td>\n","      <td>19.648989</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 190 columns</p>\n","</div>"],"text/plain":["   # Purchases  B01001001   B01001002  ...  B19001015  B19001016  B19001017\n","0           22     206252  469.226965  ...  36.440765  23.446284  21.197485\n","1            7      61399  486.538869  ...  33.000508  33.169741  24.792689\n","2            3      73170  489.859232  ...  20.125056  11.890525  16.537397\n","3           94     251724  505.585483  ...  43.731067  38.851729  40.427349\n","4            0      37382  495.586111  ...  36.118530  31.603714  19.648989\n","\n","[5 rows x 190 columns]"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"6qmELLEcI163","colab_type":"code","colab":{}},"source":["allvariablenames = list(alldata.columns.values)\n","\n","#removing first 8 (0-7) variables\n","listofallpredictors = allvariablenames[8:-1]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aFJm2wIWI3_i","colab_type":"code","colab":{}},"source":["#load predictors into dataframe\n","predictors = alldata[listofallpredictors]  \n","\n","#load target into dataframe\n","target = alldata['# Purchases']  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W98GXwMLJLtl","colab_type":"code","colab":{}},"source":["pred_train, pred_test, tar_train, tar_test = train_test_split(predictors, target, test_size=.3, random_state=123) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LOaaPg2XJwkT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"ce925af6-3b18-4d32-ed7a-54ca32ad3a10","executionInfo":{"status":"ok","timestamp":1574816254467,"user_tz":420,"elapsed":1345,"user":{"displayName":"Samuel Statton","photoUrl":"","userId":"01725452895570387843"}}},"source":["model = sklearn.linear_model.LassoLarsCV(cv=10, precompute=False).fit(pred_train, tar_train)"],"execution_count":55,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=6.800e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=3.400e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 6.322e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 47 iterations, i.e. alpha=1.839e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 66 iterations, i.e. alpha=1.279e-01, with an active set of 48 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=8.822e-02, with an active set of 64 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=8.758e-02, with an active set of 65 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=7.797e-02, with an active set of 69 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 101 iterations, i.e. alpha=7.768e-02, with an active set of 69 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 105 iterations, i.e. alpha=7.479e-02, with an active set of 73 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 110 iterations, alpha=7.275e-02, previous alpha=7.274e-02, with an active set of 77 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=4.321e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 5.960e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.244e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 45 iterations, i.e. alpha=2.242e-01, with an active set of 31 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=2.153e-01, with an active set of 35 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 51 iterations, alpha=2.031e-01, previous alpha=2.014e-01, with an active set of 36 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.639e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.216e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=8.124e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=6.648e-01, with an active set of 12 regressors, and the smallest cholesky pivot element being 6.495e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 19 iterations, i.e. alpha=4.993e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 1.490e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 27 iterations, alpha=3.881e-01, previous alpha=3.814e-01, with an active set of 24 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.671e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.233e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=8.292e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 3.161e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 13 iterations, alpha=7.394e-01, previous alpha=7.192e-01, with an active set of 12 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=8.826e-01, with an active set of 9 regressors, and the smallest cholesky pivot element being 8.297e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 18 iterations, i.e. alpha=4.413e-01, with an active set of 16 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.351e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=3.324e-01, with an active set of 26 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.675e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 3.799e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 49 iterations, i.e. alpha=1.675e-01, with an active set of 37 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 71 iterations, i.e. alpha=1.033e-01, with an active set of 55 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 84 iterations, i.e. alpha=8.377e-02, with an active set of 64 regressors, and the smallest cholesky pivot element being 4.470e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 86 iterations, alpha=8.340e-02, previous alpha=8.329e-02, with an active set of 65 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 87 iterations, i.e. alpha=6.366e-02, with an active set of 71 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 138 iterations, i.e. alpha=3.183e-02, with an active set of 104 regressors, and the smallest cholesky pivot element being 1.825e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 138 iterations, i.e. alpha=3.183e-02, with an active set of 104 regressors, and the smallest cholesky pivot element being 3.650e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 138 iterations, i.e. alpha=3.183e-02, with an active set of 104 regressors, and the smallest cholesky pivot element being 3.495e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 138 iterations, i.e. alpha=3.173e-02, with an active set of 104 regressors, and the smallest cholesky pivot element being 3.332e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 141 iterations, i.e. alpha=3.064e-02, with an active set of 107 regressors, and the smallest cholesky pivot element being 4.712e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 142 iterations, alpha=3.108e-02, previous alpha=3.035e-02, with an active set of 107 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 11 iterations, i.e. alpha=8.041e-01, with an active set of 11 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 20 iterations, i.e. alpha=4.021e-01, with an active set of 20 regressors, and the smallest cholesky pivot element being 8.363e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=3.740e-01, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 29 iterations, i.e. alpha=3.033e-01, with an active set of 27 regressors, and the smallest cholesky pivot element being 3.942e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 31 iterations, alpha=3.155e-01, previous alpha=3.002e-01, with an active set of 28 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 2 iterations, i.e. alpha=2.130e+00, with an active set of 2 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 9 iterations, i.e. alpha=1.065e+00, with an active set of 7 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 10 iterations, i.e. alpha=9.612e-01, with an active set of 8 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 12 iterations, i.e. alpha=8.103e-01, with an active set of 10 regressors, and the smallest cholesky pivot element being 6.234e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=4.776e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 6.144e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 19 iterations, alpha=4.747e-01, previous alpha=4.510e-01, with an active set of 14 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=1.441e+00, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 8 iterations, i.e. alpha=1.035e+00, with an active set of 8 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 13 iterations, i.e. alpha=7.079e-01, with an active set of 13 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 17 iterations, i.e. alpha=5.641e-01, with an active set of 15 regressors, and the smallest cholesky pivot element being 8.560e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=5.348e-01, with an active set of 19 regressors, and the smallest cholesky pivot element being 2.980e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 23 iterations, alpha=5.353e-01, previous alpha=5.348e-01, with an active set of 18 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 62 iterations, i.e. alpha=1.086e-01, with an active set of 52 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 86 iterations, i.e. alpha=5.428e-02, with an active set of 74 regressors, and the smallest cholesky pivot element being 1.054e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.714e-02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.714e-02, with an active set of 105 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.711e-02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.107e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.710e-02, with an active set of 105 regressors, and the smallest cholesky pivot element being 9.003e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=2.710e-02, with an active set of 105 regressors, and the smallest cholesky pivot element being 2.581e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=2.592e-02, with an active set of 106 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:604: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 127 iterations, alpha=2.675e-02, previous alpha=2.592e-02, with an active set of 106 regressors.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/least_angle.py:578: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 25 iterations, i.e. alpha=2.851e-01, with an active set of 23 regressors, and the smallest cholesky pivot element being 7.300e-08. Reduce max_iter or increase eps parameters.\n","  ConvergenceWarning)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"r3T7TXtvMRIL","colab_type":"text"},"source":["##Question 1:"]},{"cell_type":"code","metadata":{"id":"sv_75jt6L5fb","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"outputId":"9878b411-8ef1-4a93-f508-30f284c975a9","executionInfo":{"status":"ok","timestamp":1574816255292,"user_tz":420,"elapsed":647,"user":{"displayName":"Samuel Statton","photoUrl":"","userId":"01725452895570387843"}}},"source":["#creating a data frame based on the updated list of predictors\n","predictors_model=pd.DataFrame(listofallpredictors)\n","\n","#assigning the name 'label' to the predictors\n","predictors_model.columns = ['label']\n","\n","#creating a 'coeff' column that is assigned the coefficient values from the LassoLarsCV model \n","predictors_model['coeff'] = model.coef_\n","\n","#this for loop and if statement is going row by row (or predictor by predictor)\n","#and selecting all coefficients greater than zero and printing both the label and coefficient\n","for index, row in predictors_model.iterrows():\n","    if row['coeff'] > 0:\n","        print(row.values)\n","      "],"execution_count":56,"outputs":[{"output_type":"stream","text":["['B01001014' 1.3142095900947361]\n","['B01001036' 3.199831473110331]\n","['B01001037' 0.5867114750885728]\n","['B01001038' 1.0295045766028337]\n","['B02001005' 1.2648999446336935]\n","['B13014026' 0.8858208978839138]\n","['B13014027' 1.2507563632390104]\n","['B13016008' 0.48413572331173443]\n","['B15002015' 0.5457730397237327]\n","['B15002027' 2.01281915236849]\n","['B19001016' 0.04656593306654476]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ljlNk9scNqy6","colab_type":"text"},"source":["##Question 1 Continued:\n","\n","Predictors and their census data:\n","\n","['B01001014' 1.3142095900947361] -- Males 40-44 years old\n","\n","['B01001036' 3.199831473110331] -- Females 30-34 years old\n","\n","['B01001037' 0.5867114750885728] -- Females 35-39 years old\n","\n","['B01001038' 1.0295045766028337] -- Females 40-44 years old\n","\n","['B02001005' 1.2648999446336935] -- Race: Asian Alone\n","\n","['B13014026' 0.8858208978839138] -- Women 15-50 who have not given birth in the past 12 months, unmarried, and have a Bachelor's Degree\n","\n","\n","['B13014027' 1.2507563632390104] -- Women 15-50 who have not given birth in the past 12 months, unmarried, and have a Graduate or Profession Degree\n","\n","\n","['B13016008' 0.48413572331173443] -- Women 40-44 who have given birth in the past 12 months\n","\n","['B15002015' 0.5457730397237327] -- Males over 25 with a Bachelor's Degree\n","\n","['B15002027' 2.01281915236849] -- Females 25 or over with no diploma/12th grade highest attainment\n","\n","['B19001016' 0.04656593306654476] -- Household with income 150,000-199,999 in last 12 months"]},{"cell_type":"markdown","metadata":{"id":"1BVnhDfmMLoD","colab_type":"text"},"source":["##Question 2:\n","\n","\n","###Age & Gender Only Predictors\n","Females 30-34 (3.20) -- Meaning that we sell more Bobo Bars in areas with females 30-34 (Example you gave) but also could be the leading age and gender demographic for audience segmentation. We are also popular with males 40-44 (1.31) and are followed by females 40-44 (1.03) which indicates we sell more well in markets with high numbers of people in their early 40's. The one segment we struggle with is females 35-39 (0.587). \n","\n","###Race\n","We do well in markets with Asian populations (1.26)\n","\n","###Nuanced Age and Gender\n","When looking at females that are unmarried and have not given birth in the past 12 months, we do significantly better with women that have a Graduate/Profession Degree compared to only a Bachelor Degree (1.25 v. 0.886). We also do poorly with females 40-44 who have given birth (0.484) which is a stark difference when we just look at females 40-44 (1.03). Females over 25 with no diploma (2.01) We also sell more Bobo Bars to females over 25 with basic education while we struggle with males over 25 with a bachelors (0.546).\n","\n","###Income\n","We do poorly with household incomes between 150k and 200k. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"I-o_uAkdTaCF","colab_type":"text"},"source":["##Question 3:\n","\n","I would give my boss females 30-34 (3.20) and females with no diploma (2.01)"]},{"cell_type":"code","metadata":{"id":"9HlrEC6MTsTl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"f3400a17-47e5-4262-df91-81fdf26cbd0a","executionInfo":{"status":"ok","timestamp":1574816270715,"user_tz":420,"elapsed":490,"user":{"displayName":"Samuel Statton","photoUrl":"","userId":"01725452895570387843"}}},"source":["train_error = sklearn.metrics.mean_squared_error(tar_train, model.predict(pred_train))\n","print ('training data MSE')\n","print(train_error)"],"execution_count":57,"outputs":[{"output_type":"stream","text":["training data MSE\n","20798.061542418778\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OtNKQNcGTvDX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"f49f5743-b748-4cfb-cb04-b013235c7674","executionInfo":{"status":"ok","timestamp":1574816271678,"user_tz":420,"elapsed":731,"user":{"displayName":"Samuel Statton","photoUrl":"","userId":"01725452895570387843"}}},"source":["train_error = sklearn.metrics.mean_squared_error(tar_test, model.predict(pred_test))\n","print ('test data MSE')\n","print(train_error)"],"execution_count":58,"outputs":[{"output_type":"stream","text":["test data MSE\n","39002.73812326359\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AqhO69H0UJl1","colab_type":"text"},"source":["##Question 4:\n","The test data has a significantly higher mean squared error which means the model is not performing well with the test data. This means that the model is overfitting and causing the error to increase on the test data. "]},{"cell_type":"code","metadata":{"id":"Z39gdGnaVOj1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"004ba2d1-6f74-4c5f-9aaf-ea31922dc749","executionInfo":{"status":"ok","timestamp":1574816274080,"user_tz":420,"elapsed":490,"user":{"displayName":"Samuel Statton","photoUrl":"","userId":"01725452895570387843"}}},"source":["#r squared\n","rsquared_train=model.score(pred_train,tar_train)\n","print ('training data R-square')\n","print(rsquared_train)"],"execution_count":59,"outputs":[{"output_type":"stream","text":["training data R-square\n","0.2823739261902274\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pu42HfN9VY0f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"e28d6445-8dfc-4774-82e0-0d2acc31940a","executionInfo":{"status":"ok","timestamp":1574816274982,"user_tz":420,"elapsed":476,"user":{"displayName":"Samuel Statton","photoUrl":"","userId":"01725452895570387843"}}},"source":["#r squared\n","rsquared_train=model.score(pred_test,tar_test)\n","print ('test data R-square')\n","print(rsquared_train)"],"execution_count":60,"outputs":[{"output_type":"stream","text":["test data R-square\n","0.22637894007738157\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0t9AgAKMVe-x","colab_type":"text"},"source":["##Question 5:\n","The model only explains about 22.5% to 28% of our sales which is not very accurate. It may give insights into our most profitable customer segments, but it cannot reliably predict sales numbers solely on census data. "]},{"cell_type":"code","metadata":{"id":"_mJxC6LdVyZe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"outputId":"76f542c3-3941-48fe-f04f-285761e4fbe9","executionInfo":{"status":"ok","timestamp":1574816276656,"user_tz":420,"elapsed":505,"user":{"displayName":"Samuel Statton","photoUrl":"","userId":"01725452895570387843"}}},"source":["print(\"y interecept:\")\n","print(model.intercept_)\n","listofallpredictors[0]"],"execution_count":61,"outputs":[{"output_type":"stream","text":["y interecept:\n","130.29546325145463\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["'B01001008'"]},"metadata":{"tags":[]},"execution_count":61}]},{"cell_type":"markdown","metadata":{"id":"9RSuBpWoV6-8","colab_type":"text"},"source":["##Question 6:\n","The baseline sales is the number of purchases with the omited variable of males 20 years old. So when we read the coefficients for questions 1 & 2, the increase or decrease in sales is relative to the 20 year old male segment. "]}]}